{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP_Assignment-1_Question-3 Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MModUAQ_IuZG"
      },
      "source": [
        "1. Write a program to enter a string from the user and perform the following tasks:\n",
        "    * Write a python function named \"Tokenize\" which returns the tokenized string\n",
        "    * Print tokens along with the frequency of each token using the above function\n",
        "    * Print the 5 least occurring tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwgHOcuTQ93n",
        "outputId": "effe0cf7-b1ea-4336-ba5e-5d174b8d8562"
      },
      "source": [
        "#import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from operator import itemgetter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw_hbIfIIuZM",
        "outputId": "7b055e67-fcc0-4c62-9c65-27c7927dde2b"
      },
      "source": [
        "%%writefile tokenizer_func.py\n",
        "\n",
        "#import libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from operator import itemgetter\n",
        "\n",
        "#define tokenize function\n",
        "def Tokenize():\n",
        "    text = input()\n",
        "    tokens = word_tokenize(text) #tokenize the words\n",
        "    fdist = FreqDist(tokens) #find the frequency of words\n",
        "    for f in fdist:\n",
        "        print(f, fdist[f])\n",
        "    min_5 = dict(sorted(fdist.items(), key=itemgetter(1))[:5]) #find the least 5 occuring words\n",
        "    print(\"Least 5 occurring tokens:\", min_5)\n",
        "    return tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tokenizer_func.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYfBIhpvMAjR",
        "outputId": "6714be51-781c-484e-c67b-4316ecd07e91"
      },
      "source": [
        "#run the Tokenize function\n",
        "from tokenizer_func import Tokenize\n",
        "Tokenize()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
            "Natural 1\n",
            "language 3\n",
            "processing 1\n",
            "( 1\n",
            "NLP 1\n",
            ") 1\n",
            "is 1\n",
            "a 1\n",
            "subfield 1\n",
            "of 2\n",
            "linguistics 1\n",
            ", 3\n",
            "computer 1\n",
            "science 1\n",
            "and 3\n",
            "artificial 1\n",
            "intelligence 1\n",
            "concerned 1\n",
            "with 1\n",
            "the 1\n",
            "interactions 1\n",
            "between 1\n",
            "computers 2\n",
            "human 1\n",
            "in 1\n",
            "particular 1\n",
            "how 1\n",
            "to 2\n",
            "program 1\n",
            "process 1\n",
            "analyze 1\n",
            "large 1\n",
            "amounts 1\n",
            "natural 1\n",
            "data 1\n",
            ". 1\n",
            "Least 5 occurring tokens: {'Natural': 1, 'processing': 1, '(': 1, 'NLP': 1, ')': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'linguistics',\n",
              " ',',\n",
              " 'computer',\n",
              " 'science',\n",
              " ',',\n",
              " 'and',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'the',\n",
              " 'interactions',\n",
              " 'between',\n",
              " 'computers',\n",
              " 'and',\n",
              " 'human',\n",
              " 'language',\n",
              " ',',\n",
              " 'in',\n",
              " 'particular',\n",
              " 'how',\n",
              " 'to',\n",
              " 'program',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'process',\n",
              " 'and',\n",
              " 'analyze',\n",
              " 'large',\n",
              " 'amounts',\n",
              " 'of',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4Ffh1ZIuZN"
      },
      "source": [
        "2. Write a program to enter a string from the user and perform the following tasks\n",
        "    * Write a python function named \"RemoveStopWords\" which returns the string after removing stop words\n",
        "    * Count frequency of each stop word present in a string using the above function\n",
        "    * Plot a bar graph depicting stop words and their frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nEsJKksIuZN",
        "outputId": "c800ff20-98eb-4a18-987a-55d21ee1292f"
      },
      "source": [
        "%%writefile removestopwords.py\n",
        "#Remove stop words function\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "def RemoveStopWords(token_list):\n",
        "    stop_words = set(stopwords.words('english')) #copy stopwords to a list\n",
        "    stop_removed = [w for w in token_list if w not in stop_words] #for loop to append the words to a list except stopwords\n",
        "    return stop_removed"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing removestopwords.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQOKG4vmIuZN",
        "outputId": "6f7d3b2c-4788-44c3-bb58-b9fffca1d766"
      },
      "source": [
        "#counting frequency of stop words in a string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "string = input()\n",
        "from removestopwords import RemoveStopWords\n",
        "\n",
        "tokens = word_tokenize(string) #tokenize\n",
        "stop_removed = RemoveStopWords(tokens) #remove stopwords\n",
        "\n",
        "stop_present = [word for word in tokens if word not in stop_removed] #save stop words to a list\n",
        "print(stop_present,'\\n==========================================')\n",
        "\n",
        "swfdist = FreqDist(stop_present) #find the frequency of stop words\n",
        "print(\"Frequency Distribution of Stop Words:\\n==========================================\")\n",
        "for f in swfdist:\n",
        "    print(\"{}: {}\".format(f, swfdist[f]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['is', 'a', 'of', 'and', 'with', 'the', 'between', 'and', 'in', 'how', 'to', 'to', 'and', 'of'] \n",
            "==========================================\n",
            "Frequency Distribution of Stop Words:\n",
            "==========================================\n",
            "is: 1\n",
            "a: 1\n",
            "of: 2\n",
            "and: 3\n",
            "with: 1\n",
            "the: 1\n",
            "between: 1\n",
            "in: 1\n",
            "how: 1\n",
            "to: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Nxq62hVSIuZO",
        "outputId": "647bc553-3c3e-4a77-b08e-f09f7dfcae36"
      },
      "source": [
        "#plot a bar graph depicting frequency of stop words\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(swfdist.keys(), swfdist.values())\n",
        "plt.title(\"Frequency of Stop words in the input\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Frequency of Stop words in the input')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xySAQtgAIuZO"
      },
      "source": [
        "3. Write a program to enter a string from the user and perform the following tasks\n",
        "    * Write a python function named \"Lemmatize\" which returns a string after lemmatizing the string\n",
        "    * Write a python function named \"Stemmed\" which returns a string after stemming the string\n",
        "    * Print all the words along with their lemmatized and stemmed from using the above functions\n",
        "    * Save these results in a CSV file having 3 columns: Original Word, Lemmatized Form, Stemmed Form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBNlAL20IuZO",
        "outputId": "69a9d91e-f07b-4fda-dbd0-13528f6702a8"
      },
      "source": [
        "%%writefile lemma_stem.py\n",
        "#lemmatize and stemming\n",
        "#import libraries\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "#define lemmatize function\n",
        "def Lemmatize(token_list):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in token_list]\n",
        "    return lemmatized\n",
        "\n",
        "#define stemming function\n",
        "def Stemmed(token_list):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in token_list]\n",
        "    return stemmed"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing lemma_stem.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jUaj78xIuZP",
        "outputId": "f646d975-69d3-4cc4-bc03-bf6d5bd00295"
      },
      "source": [
        "#implement lemmatizer and stemmer\n",
        "text = \"cries studies studied cried hurry hurried hurries jump jumped jumping jumps\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "import lemma_stem\n",
        "from lemma_stem import Lemmatize, Stemmed\n",
        "lemma = Lemmatize(tokens)\n",
        "stem = Stemmed(tokens)\n",
        "\n",
        "#write the results to a Pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df['Original Word'] = tokens\n",
        "df['Lemmatized Form'] = lemma\n",
        "df['Stemmed Form'] = stem\n",
        "\n",
        "print(df)\n",
        "df.to_csv(\"StemmedAndLemmatizedResults.csv\") #save the results to CSV file"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "   Original Word Lemmatized Form Stemmed Form\n",
            "0          cries             cry          cri\n",
            "1        studies           study        studi\n",
            "2        studied         studied        studi\n",
            "3          cried           cried          cri\n",
            "4          hurry           hurry        hurri\n",
            "5        hurried         hurried        hurri\n",
            "6        hurries           hurry        hurri\n",
            "7           jump            jump         jump\n",
            "8         jumped          jumped         jump\n",
            "9        jumping         jumping         jump\n",
            "10         jumps            jump         jump\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqv8s8BUIuZP"
      },
      "source": [
        "4. Create a python file named \"PreProcess\" and perform the following tasks:\n",
        "    * Copy the function \"Tokenize\" in this file from question1\n",
        "    * Copy the function \"RemoveStopWords\" in this file from question2\n",
        "    * Copy the function \"Lemmatize\" in this file from question3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPXFyiFDIuZP"
      },
      "source": [
        "Create a function named \"Refine\" which accepts a string and calls the above 3 functions in the same order\n",
        "    * Tokenize (inputted string will be the input)\n",
        "    * RemoveStopWords (tokenized string should be input)\n",
        "    * Lemmatize (stop words removed string shoulg be input)\n",
        "    \n",
        "Save this python file as PreProcess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHVc5RIbIuZP",
        "outputId": "78ae79a3-e252-4cb6-b7f2-9f6e06205322"
      },
      "source": [
        "%%writefile PreProcess.py\n",
        "#creating a PreProcess file\n",
        "from tokenizer_func import Tokenize\n",
        "from removestopwords import RemoveStopWords\n",
        "from lemma_stem import Lemmatize\n",
        "\n",
        "#defining Refine function to carry out the processing\n",
        "def Refine():\n",
        "    tokens = Tokenize()\n",
        "    stop_remove = RemoveStopWords(tokens)\n",
        "    lemmatized_words = Lemmatize(stop_remove)\n",
        "    print('Tokenized: ', tokens)\n",
        "    print('Stop Words Removed Copy: ', stop_remove)\n",
        "    print('Lemmatized Words: ', lemmatized_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing PreProcess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK0-gyIaJ8l5",
        "outputId": "5234e006-ee4a-42fa-81a7-7165dc29ff81"
      },
      "source": [
        "#runt the refine function\n",
        "from PreProcess import Refine\n",
        "Refine()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
            "Natural 1\n",
            "language 3\n",
            "processing 1\n",
            "( 1\n",
            "NLP 1\n",
            ") 1\n",
            "is 1\n",
            "a 1\n",
            "subfield 1\n",
            "of 2\n",
            "linguistics 1\n",
            ", 3\n",
            "computer 1\n",
            "science 1\n",
            "and 3\n",
            "artificial 1\n",
            "intelligence 1\n",
            "concerned 1\n",
            "with 1\n",
            "the 1\n",
            "interactions 1\n",
            "between 1\n",
            "computers 2\n",
            "human 1\n",
            "in 1\n",
            "particular 1\n",
            "how 1\n",
            "to 2\n",
            "program 1\n",
            "process 1\n",
            "analyze 1\n",
            "large 1\n",
            "amounts 1\n",
            "natural 1\n",
            "data 1\n",
            ". 1\n",
            "Least 5 occurring tokens: {'Natural': 1, 'processing': 1, '(': 1, 'NLP': 1, ')': 1}\n",
            "Tokenized:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.']\n",
            "Stop Words Removed Copy:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.']\n",
            "Lemmatized Words:  ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}